{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "id": "-NNVysCYsih3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy pandas matplotlib scipy tensorflow onnx tqdm wfdb albumentations wfdb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bsHrKBFswBB",
        "outputId": "2ea6d74f-142a-462d-8cf8-eebf292b9f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the data"
      ],
      "metadata": {
        "id": "iOecsLaWuu63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cOvUsJ3GvK3H",
        "outputId": "89544fd0-a3e9-4e7e-f5f0-1b591fc26fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load ECG data\n",
        "def load_ecg_record(record_name, data_dir='/content/'):\n",
        "    \"\"\"\n",
        "    Load ECG record from WFDB format.\n",
        "\n",
        "    Args:\n",
        "        record_name: Name of the record (e.g., '100')\n",
        "        data_dir: Directory containing the data files\n",
        "\n",
        "    Returns:\n",
        "        record: WFDB record object\n",
        "        signals: ECG signals\n",
        "        annotations: ECG annotations\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load record\n",
        "        record = wfdb.rdrecord(f\"{data_dir}{record_name}\")\n",
        "\n",
        "        # Load annotations\n",
        "        ann = wfdb.rdann(f\"{data_dir}{record_name}\", 'atr')\n",
        "\n",
        "        return record, record.p_signal, ann\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading record {record_name}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# List of available records (first few for demonstration)\n",
        "record_names = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
        "\n",
        "print(\"📁 Available records:\")\n",
        "for name in record_names:\n",
        "    print(f\"   - {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4J-MxCzuy48",
        "outputId": "f1af4982-db27-4be4-80b2-7481a2761818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Available records:\n",
            "   - 100\n",
            "   - 101\n",
            "   - 102\n",
            "   - 103\n",
            "   - 104\n",
            "   - 105\n",
            "   - 106\n",
            "   - 107\n",
            "   - 108\n",
            "   - 109\n",
            "   - 111\n",
            "   - 112\n",
            "   - 113\n",
            "   - 114\n",
            "   - 115\n",
            "   - 116\n",
            "   - 117\n",
            "   - 118\n",
            "   - 119\n",
            "   - 121\n",
            "   - 122\n",
            "   - 123\n",
            "   - 124\n",
            "   - 200\n",
            "   - 201\n",
            "   - 202\n",
            "   - 203\n",
            "   - 205\n",
            "   - 207\n",
            "   - 208\n",
            "   - 209\n",
            "   - 210\n",
            "   - 212\n",
            "   - 213\n",
            "   - 214\n",
            "   - 215\n",
            "   - 217\n",
            "   - 219\n",
            "   - 220\n",
            "   - 221\n",
            "   - 222\n",
            "   - 223\n",
            "   - 228\n",
            "   - 230\n",
            "   - 231\n",
            "   - 232\n",
            "   - 233\n",
            "   - 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "import scipy\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atES-nCFvDeT",
        "outputId": "884713b2-66e9-4098-df02-d9e16db8184d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a sample record\n",
        "record_name = '100'  # This is a common record with good signal quality\n",
        "record, signals, annotations = load_ecg_record(record_name)\n",
        "\n",
        "if record is not None:\n",
        "    print(f\"✅ Successfully loaded record {record_name}\")\n",
        "    print(f\"   Duration: {len(signals) / record.fs / 60:.1f} minutes\")\n",
        "    print(f\"   Sampling rate: {record.fs} Hz\")\n",
        "    print(f\"   Number of leads: {signals.shape[1]}\")\n",
        "    print(f\"   Signal shape: {signals.shape}\")\n",
        "    print(f\"   Number of annotations: {len(annotations.sample)}\")\n",
        "\n",
        "    # Get lead names\n",
        "    print(f\"   Lead names: {record.sig_name}\")\n",
        "else:\n",
        "    print(\"⚠️  Using simulated data for demonstration\")\n",
        "    # Create simulated ECG data for demonstration\n",
        "    fs = 360  # Hz\n",
        "    duration = 60  # seconds\n",
        "    t = np.linspace(0, duration, duration * fs)\n",
        "\n",
        "    # Simulate normal sinus rhythm\n",
        "    heart_rate = 75  # BPM\n",
        "    rr_interval = 60 / heart_rate\n",
        "    r_peaks = np.arange(0, duration, rr_interval)\n",
        "\n",
        "    ecg = np.zeros_like(t)\n",
        "    for r_peak in r_peaks:\n",
        "        r_idx = int(r_peak * fs)\n",
        "        if r_idx < len(ecg):\n",
        "            ecg[r_idx] = 1.0\n",
        "\n",
        "            # Add QRS complex\n",
        "            qrs_width = int(0.1 * fs)\n",
        "            start_idx = max(0, r_idx - qrs_width // 2)\n",
        "            end_idx = min(len(ecg), r_idx + qrs_width // 2)\n",
        "\n",
        "            if start_idx < r_idx:\n",
        "                ecg[start_idx:r_idx] = -0.1\n",
        "            if r_idx < end_idx:\n",
        "                ecg[r_idx:end_idx] = -0.2\n",
        "\n",
        "    # Add noise and baseline wander\n",
        "    baseline = 0.1 * np.sin(2 * np.pi * 0.1 * t)\n",
        "    noise = 0.05 * np.random.randn(len(t))\n",
        "    ecg = ecg + baseline + noise\n",
        "\n",
        "    signals = ecg.reshape(-1, 1)\n",
        "    record = type('Record', (), {'fs': fs, 'sig_name': ['MLII']})()\n",
        "    annotations = type('Annotations', (), {'sample': np.array([1000, 2000, 3000])})()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQfawbAuwsW",
        "outputId": "3782733e-d292-414f-b2ff-5d4493879917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded record 100\n",
            "   Duration: 30.1 minutes\n",
            "   Sampling rate: 360 Hz\n",
            "   Number of leads: 2\n",
            "   Signal shape: (650000, 2)\n",
            "   Number of annotations: 2274\n",
            "   Lead names: ['MLII', 'V5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data (Windowing, Patching, and Linear Projection)\n",
        "\n",
        "\n",
        "1.   Windowing\n",
        "2.   Patching\n",
        "3.   Linear Projection and Positional Encoding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiPr1Qi9ssTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "window_size = 1800  # 5 seconds at 360Hz\n",
        "stride = 1800       # No overlap\n",
        "\n",
        "ecg_signal = signals.flatten()\n",
        "windows = [ecg_signal[i:i+window_size] for i in range(0, len(ecg_signal), stride)]\n",
        "\n",
        "\n",
        "#Checking the size of each window\n",
        "len(windows[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVswfsvLuoa3",
        "outputId": "7d2619c5-c115-4f64-b665-6d86a2677ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "window_size = 1800  # For example, 5 seconds at 360Hz = 1800 samples per window\n",
        "patch_size = 20  # Each patch contains 20 samples\n",
        "\n",
        "# Step 1: Segment the ECG signal into windows\n",
        "windows = [ecg_signal[i:i+window_size] for i in range(0, len(ecg_signal), window_size)]\n",
        "print(len(windows[0]))\n",
        "\n",
        "# Step 2: Convert each window into patches\n",
        "patches = []\n",
        "\n",
        "for window in windows:\n",
        "    # Divide the window into smaller patches (e.g., of size 20)\n",
        "    window_patches = [window[i:i+patch_size] for i in range(0, len(window), patch_size)]\n",
        "    patches.extend(window_patches)  # Add the patches to the main list\n",
        "\n",
        "# Step 3: Convert patches to a PyTorch tensor\n",
        "patches_tensor = torch.tensor(patches, dtype=torch.float32)\n",
        "\n",
        "# Verify the shape of the patches tensor\n",
        "print(patches_tensor.shape)  # Should print torch.Size([num_patches, patch_size])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4nInBhPv38o",
        "outputId": "ac01ddc4-bc9b-44cf-dce8-853d71ff600d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800\n",
            "torch.Size([65000, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearProjection(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearProjection, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Example: linear projection from 20 samples (input) to 32-dimensional vectors (output)\n",
        "patch_embedding = LinearProjection(20, 32)\n",
        "embeddings = patch_embedding(torch.tensor(patches_tensor, dtype=torch.float32))\n",
        "\n",
        "\n",
        "\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "tFxuMFE4yYMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b424b17d-0ae5-46a1-94b8-58d935fc1d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([65000, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, patch_size, embed_size, num_heads, num_layers, num_classes):\n",
        "        super(TinyTransformer, self).__init__()\n",
        "\n",
        "        self.patch_embedding = LinearProjection(patch_size, embed_size)\n",
        "        # Changed positional encoding to match batch processing\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 90, embed_size))  # 90 patches per window\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embed_size,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=128)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(embed_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, patch_size] -> [32, 20]\n",
        "\n",
        "        # 1. Apply patch embedding\n",
        "        x = self.patch_embedding(x)  # [32, 20] -> [32, embed_size]\n",
        "\n",
        "        # 2. Reshape and add positional encoding\n",
        "        x = x.unsqueeze(1)  # [32, embed_size] -> [32, 1, embed_size]\n",
        "        x = x + self.positional_encoding[:, :1, :]  # Add positional encoding for first position\n",
        "\n",
        "        # 3. Transformer expects [seq_len, batch_size, embed_size]\n",
        "        x = x.transpose(0, 1)  # [1, 32, embed_size]\n",
        "\n",
        "        # 4. Pass through transformer layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # 5. Get back to [batch_size, embed_size]\n",
        "        x = x.transpose(0, 1)  # [32, 1, embed_size]\n",
        "        x = x.squeeze(1)  # [32, embed_size]\n",
        "\n",
        "        # 6. Final classification\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "vq7GqLZk6Afa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, signals, annotations, patch_size=20, window_size=1800, fs=360):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            signals (numpy.array or list): ECG signal (shape: [num_windows, window_size])\n",
        "            annotations (wfdb.Annotation): Annotations corresponding to the signals\n",
        "            patch_size (int): Size of each patch (default is 20)\n",
        "            window_size (int): Size of each window (default is 1800)\n",
        "            fs (int): Sampling frequency (default 360 Hz)\n",
        "        \"\"\"\n",
        "        self.signals = signals\n",
        "        self.annotations = annotations\n",
        "        self.patch_size = patch_size\n",
        "        self.window_size = window_size\n",
        "        self.fs = fs  # Sampling frequency\n",
        "        self.patches, self.labels = self._create_patches_and_labels()\n",
        "\n",
        "    def _create_patches_and_labels(self):\n",
        "        patches = []\n",
        "        labels = []\n",
        "\n",
        "        # Map annotation symbols to numerical labels\n",
        "        annotation_labels = {\n",
        "            'N': 0,  # Normal beat\n",
        "            'V': 1,  # Premature ventricular contraction (PVC)\n",
        "            'A': 2,  # Atrial premature beat\n",
        "            'F': 3,  # Fusion beat\n",
        "            'L': 4   # Left bundle branch block\n",
        "            # Add other types as needed\n",
        "        }\n",
        "\n",
        "        # For each signal window\n",
        "        for window_idx, window in enumerate(self.signals):\n",
        "            window_start_sample = window_idx * self.window_size\n",
        "            window_end_sample = window_start_sample + self.window_size\n",
        "\n",
        "            # Find annotations that fall within this window\n",
        "            window_labels = []\n",
        "\n",
        "            for i, sample in enumerate(self.annotations.sample):\n",
        "                if window_start_sample <= sample < window_end_sample:\n",
        "                    # Map annotation to label\n",
        "                    symbol = self.annotations.symbol[i]\n",
        "                    if symbol in annotation_labels:\n",
        "                        window_labels.append(annotation_labels[symbol])\n",
        "\n",
        "            # If the window has any labels, use the majority as the label for the window\n",
        "            if window_labels:\n",
        "                label = max(set(window_labels), key=window_labels.count)  # Majority label in this window\n",
        "            else:\n",
        "                label = 0  # Default to normal if no annotations found in the window\n",
        "\n",
        "            # Split the window into patches\n",
        "            for i in range(0, len(window), self.patch_size):\n",
        "                patch = window[i:i + self.patch_size]\n",
        "                if len(patch) == self.patch_size:\n",
        "                    patches.append(patch)\n",
        "                    labels.append(label)\n",
        "\n",
        "        return patches, labels\n",
        "# Usage example:\n",
        "# Assuming 'signals' is your ECG data loaded earlier\n",
        "dataset = ECGDataset(signals, annotations = annotations)  # Flatten if multi-lead\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example of using the dataloader\n",
        "for batch in dataloader:\n",
        "    patches, labels = batch\n",
        "    print(f\"Batch patches shape: {patches.shape}\")  # Should be [32, 20]\n",
        "    print(f\"Batch labels shape: {labels.shape}\")    # Should be [32]\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ob0_FcTY6_-f",
        "outputId": "8d3d6055-7ee7-478c-e8c1-8c3ee3af4f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'ECGDataset' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-91-1856211530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Assuming 'signals' is your ECG data loaded earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten if multi-lead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Example of using the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m             )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             raise ValueError(\n\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36mnum_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# dataset size might change at runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'ECGDataset' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# model = TinyTransformer(patch_size=20, embed_size=32, num_heads=4, num_layers=2, num_classes=5)\n",
        "\n",
        "# # Loss and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# epochs = 1\n",
        "\n",
        "# # Training loop (simplified)\n",
        "# for epoch in range(epochs):\n",
        "#     for batch in dataloader:\n",
        "#         ecg_data, labels = batch\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(ecg_data)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backpropagate and optimize\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "k22V53Y-6Fk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prepare the Dataset (using the DataLoader we created earlier)\n",
        "dataset = ECGDataset(signals.flatten())  # Assuming 'signals' is your ECG data\n",
        "\n",
        "# Split into train and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Initialize Model, Loss, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TinyTransformer(\n",
        "    patch_size=20,\n",
        "    embed_size=32,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "# 3. Training Loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for patches, labels in train_loader:\n",
        "        patches, labels = patches.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(patches)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate training metrics\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for patches, labels in val_loader:\n",
        "            patches, labels = patches.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(patches)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# 4. Save Final Model\n",
        "torch.save(model.state_dict(), 'final_model.pth')\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2dbW0pa7Ofd",
        "outputId": "76e8d132-3902-4a20-c000-610c4c41547b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "  Train Loss: 1.6145 | Acc: 0.2073 | F1: 0.2072\n",
            "  Val Loss: 1.6100 | Acc: 0.2024 | F1: 0.0841\n",
            "------------------------------------------------------------\n",
            "Epoch 2/10:\n",
            "  Train Loss: 1.6095 | Acc: 0.2050 | F1: 0.2043\n",
            "  Val Loss: 1.6111 | Acc: 0.2025 | F1: 0.0815\n",
            "------------------------------------------------------------\n",
            "Epoch 3/10:\n",
            "  Train Loss: 1.6088 | Acc: 0.2016 | F1: 0.2007\n",
            "  Val Loss: 1.6077 | Acc: 0.2075 | F1: 0.0805\n",
            "------------------------------------------------------------\n",
            "Epoch 4/10:\n",
            "  Train Loss: 1.6080 | Acc: 0.2049 | F1: 0.2044\n",
            "  Val Loss: 1.6079 | Acc: 0.2067 | F1: 0.1104\n",
            "------------------------------------------------------------\n",
            "Epoch 5/10:\n",
            "  Train Loss: 1.6079 | Acc: 0.2034 | F1: 0.2026\n",
            "  Val Loss: 1.6065 | Acc: 0.2090 | F1: 0.0947\n",
            "------------------------------------------------------------\n",
            "Epoch 6/10:\n",
            "  Train Loss: 1.6077 | Acc: 0.2019 | F1: 0.1976\n",
            "  Val Loss: 1.6071 | Acc: 0.2015 | F1: 0.0887\n",
            "------------------------------------------------------------\n",
            "Epoch 7/10:\n",
            "  Train Loss: 1.6073 | Acc: 0.2044 | F1: 0.2036\n",
            "  Val Loss: 1.6076 | Acc: 0.1995 | F1: 0.0749\n",
            "------------------------------------------------------------\n",
            "Epoch 8/10:\n",
            "  Train Loss: 1.6073 | Acc: 0.2050 | F1: 0.2019\n",
            "  Val Loss: 1.6058 | Acc: 0.2053 | F1: 0.1332\n",
            "------------------------------------------------------------\n",
            "Epoch 9/10:\n",
            "  Train Loss: 1.6068 | Acc: 0.2032 | F1: 0.2029\n",
            "  Val Loss: 1.6086 | Acc: 0.2031 | F1: 0.1249\n",
            "------------------------------------------------------------\n",
            "Epoch 10/10:\n",
            "  Train Loss: 1.6078 | Acc: 0.2030 | F1: 0.1926\n",
            "  Val Loss: 1.6060 | Acc: 0.1996 | F1: 0.0776\n",
            "------------------------------------------------------------\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny Transformer Model\n",
        "\n",
        "\n",
        "Model Components\n",
        "\n",
        "Multi-head self-attention layer\n",
        "\n",
        "Feed-forward network\n",
        "\n",
        "Layer normalization + residual connections\n",
        "\n",
        "Final classification layer"
      ],
      "metadata": {
        "id": "YxSDsyXAtAHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRZ7xPEssS2F"
      },
      "outputs": [],
      "source": []
    }
  ]
}